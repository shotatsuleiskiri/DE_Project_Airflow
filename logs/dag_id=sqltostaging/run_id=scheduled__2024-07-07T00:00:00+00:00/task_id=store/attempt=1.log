[2024-07-08T18:13:47.026+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:13:47.058+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:13:47.065+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-07-08T18:13:47.095+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): store> on 2024-07-07 00:00:00+00:00
[2024-07-08T18:13:47.102+0000] {standard_task_runner.py:60} INFO - Started process 132 to run task
[2024-07-08T18:13:47.106+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'SqlToStaging', 'store', 'scheduled__2024-07-07T00:00:00+00:00', '--job-id', '43', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmp5t6brz1t']
[2024-07-08T18:13:47.145+0000] {standard_task_runner.py:88} INFO - Job 43: Subtask store
[2024-07-08T18:13:47.268+0000] {task_command.py:423} INFO - Running <TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [running]> on host 091922596b62
[2024-07-08T18:13:47.410+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='SqlToStaging' AIRFLOW_CTX_TASK_ID='store' AIRFLOW_CTX_EXECUTION_DATE='2024-07-07T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-07T00:00:00+00:00'
[2024-07-08T18:13:47.412+0000] {logging_mixin.py:188} INFO - dvtobv :dag_id
[2024-07-08T18:13:47.416+0000] {logging_mixin.py:188} INFO - table: store
[2024-07-08T18:13:47.417+0000] {logging_mixin.py:188} INFO - 
 stage: SqlToStaging 
 table:store
[2024-07-08T18:13:47.417+0000] {logging_mixin.py:188} INFO - select *  from etlconf.Etl_Process_Mapping where SourceTableName = 'store' and Stage = 'SqlToStaging'
[2024-07-08T18:13:47.486+0000] {logging_mixin.py:188} INFO - 0    dvdrental
Name: sourcedbname, dtype: object 0    store
Name: sourcetablename, dtype: object 0    public
Name: sourceschema, dtype: object
[2024-07-08T18:13:47.531+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag.py", line 22, in process_table
    return  execute(table, stage)
  File "/app/main.py", line 25, in execute
    SqlToStaging().create_full().some_function(table, stage)
  File "/app/concretestage/SQLToStaging.py", line 34, in some_function
    sourceDF = getDF(df['sourcedbname'], df['sourcetablename'], df['sourceschema'])
  File "/app/myFramework/utils/utils.py", line 22, in getDF
    cur = conn.getCursor(source_dbname)
  File "/app/myFramework/source/posgresql/connect.py", line 6, in getCursor
    conn = psycopg2.connect(database = dbname,
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres_db" (192.168.0.3), port 9999 failed: FATAL:  database "0    dvdrental
Name: sourcedbname, dtype: object" does not exist

[2024-07-08T18:13:47.818+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=SqlToStaging, task_id=store, execution_date=20240707T000000, start_date=20240708T181347, end_date=20240708T181347
[2024-07-08T18:13:47.834+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 43 for task store (connection to server at "postgres_db" (192.168.0.3), port 9999 failed: FATAL:  database "0    dvdrental
Name: sourcedbname, dtype: object" does not exist
; 132)
[2024-07-08T18:13:47.891+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-07-08T18:13:47.941+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-08T18:17:21.313+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:17:21.355+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:17:21.366+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-07-08T18:17:21.438+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): store> on 2024-07-07 00:00:00+00:00
[2024-07-08T18:17:21.466+0000] {standard_task_runner.py:60} INFO - Started process 81 to run task
[2024-07-08T18:17:21.467+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'SqlToStaging', 'store', 'scheduled__2024-07-07T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmp4t05veo0']
[2024-07-08T18:17:21.487+0000] {standard_task_runner.py:88} INFO - Job 4: Subtask store
[2024-07-08T18:17:21.706+0000] {task_command.py:423} INFO - Running <TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [running]> on host 0c73c124d202
[2024-07-08T18:17:21.920+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='SqlToStaging' AIRFLOW_CTX_TASK_ID='store' AIRFLOW_CTX_EXECUTION_DATE='2024-07-07T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-07T00:00:00+00:00'
[2024-07-08T18:17:21.925+0000] {logging_mixin.py:188} INFO - dvtobv :dag_id
[2024-07-08T18:17:21.929+0000] {logging_mixin.py:188} INFO - table: store
[2024-07-08T18:17:21.930+0000] {logging_mixin.py:188} INFO - 
 stage: SqlToStaging 
 table:store
[2024-07-08T18:17:21.938+0000] {logging_mixin.py:188} INFO - select *  from etlconf.Etl_Process_Mapping where SourceTableName = 'store' and Stage = 'SqlToStaging'
[2024-07-08T18:17:22.109+0000] {logging_mixin.py:188} INFO - 0    dvdrental
Name: sourcedbname, dtype: object 0    store
Name: sourcetablename, dtype: object 0    public
Name: sourceschema, dtype: object
[2024-07-08T18:17:22.146+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag.py", line 22, in process_table
    return  execute(table, stage)
  File "/app/main.py", line 25, in execute
    SqlToStaging().create_full().some_function(table, stage)
  File "/app/concretestage/SQLToStaging.py", line 34, in some_function
    sourceDF = getDF(df['sourcedbname'], df['sourcetablename'], df['sourceschema'])
  File "/app/myFramework/utils/utils.py", line 22, in getDF
    cur = conn.getCursor(source_dbname)
  File "/app/myFramework/source/posgresql/connect.py", line 6, in getCursor
    conn = psycopg2.connect(database = dbname,
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres_db" (172.18.0.5), port 9999 failed: FATAL:  database "0    dvdrental
Name: sourcedbname, dtype: object" does not exist

[2024-07-08T18:17:23.947+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=SqlToStaging, task_id=store, execution_date=20240707T000000, start_date=20240708T181721, end_date=20240708T181723
[2024-07-08T18:17:24.025+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 4 for task store (connection to server at "postgres_db" (172.18.0.5), port 9999 failed: FATAL:  database "0    dvdrental
Name: sourcedbname, dtype: object" does not exist
; 81)
[2024-07-08T18:17:24.098+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-07-08T18:17:24.216+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-08T18:19:48.888+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:19:48.899+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:19:48.903+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-07-08T18:19:48.929+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): store> on 2024-07-07 00:00:00+00:00
[2024-07-08T18:19:48.952+0000] {standard_task_runner.py:60} INFO - Started process 96 to run task
[2024-07-08T18:19:48.970+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'SqlToStaging', 'store', 'scheduled__2024-07-07T00:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmp36w8a_dj']
[2024-07-08T18:19:48.996+0000] {standard_task_runner.py:88} INFO - Job 11: Subtask store
[2024-07-08T18:19:49.110+0000] {task_command.py:423} INFO - Running <TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [running]> on host 987b4d541fd1
[2024-07-08T18:19:50.120+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='SqlToStaging' AIRFLOW_CTX_TASK_ID='store' AIRFLOW_CTX_EXECUTION_DATE='2024-07-07T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-07T00:00:00+00:00'
[2024-07-08T18:19:50.121+0000] {logging_mixin.py:188} INFO - dvtobv :dag_id
[2024-07-08T18:19:50.123+0000] {logging_mixin.py:188} INFO - table: store
[2024-07-08T18:19:50.133+0000] {logging_mixin.py:188} INFO - 
 stage: SqlToStaging 
 table:store
[2024-07-08T18:19:50.135+0000] {logging_mixin.py:188} INFO - select *  from etlconf.Etl_Process_Mapping where SourceTableName = 'store' and Stage = 'SqlToStaging'
[2024-07-08T18:19:50.162+0000] {logging_mixin.py:188} INFO - 0    dvdrental
Name: sourcedbname, dtype: object 0    store
Name: sourcetablename, dtype: object 0    public
Name: sourceschema, dtype: object
[2024-07-08T18:19:50.163+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-07-08T18:19:50.178+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=SqlToStaging, task_id=store, execution_date=20240707T000000, start_date=20240708T181948, end_date=20240708T181950
[2024-07-08T18:19:50.233+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-07-08T18:19:50.294+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-08T18:26:43.431+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:26:43.453+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:26:43.458+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-07-08T18:26:43.492+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): store> on 2024-07-07 00:00:00+00:00
[2024-07-08T18:26:43.502+0000] {standard_task_runner.py:60} INFO - Started process 89 to run task
[2024-07-08T18:26:43.520+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'SqlToStaging', 'store', 'scheduled__2024-07-07T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpeekb3veo']
[2024-07-08T18:26:43.525+0000] {standard_task_runner.py:88} INFO - Job 3: Subtask store
[2024-07-08T18:26:43.651+0000] {task_command.py:423} INFO - Running <TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [running]> on host 7f814bc03799
[2024-07-08T18:26:43.989+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='SqlToStaging' AIRFLOW_CTX_TASK_ID='store' AIRFLOW_CTX_EXECUTION_DATE='2024-07-07T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-07T00:00:00+00:00'
[2024-07-08T18:26:43.994+0000] {logging_mixin.py:188} INFO - dvtobv :dag_id
[2024-07-08T18:26:43.998+0000] {logging_mixin.py:188} INFO - table: store
[2024-07-08T18:26:44.035+0000] {logging_mixin.py:188} INFO - 
 stage: SqlToStaging 
 table:store
[2024-07-08T18:26:44.041+0000] {logging_mixin.py:188} INFO - select *  from etlconf.Etl_Process_Mapping where SourceTableName = 'store' and Stage = 'SqlToStaging'
[2024-07-08T18:26:44.329+0000] {logging_mixin.py:188} INFO - 0    dvdrental
Name: sourcedbname, dtype: object 0    store
Name: sourcetablename, dtype: object 0    public
Name: sourceschema, dtype: object
[2024-07-08T18:26:44.368+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag.py", line 22, in process_table
    return  execute(table, stage)
  File "/app/main.py", line 25, in execute
    SqlToStaging().create_full().some_function(table, stage)
  File "/app/concretestage/SQLToStaging.py", line 34, in some_function
    sourceDF = getDF(df['sourcedbname'], df['sourcetablename'], df['sourceschema'])
  File "/app/myFramework/utils/utils.py", line 22, in getDF
    cur = conn.getCursor(source_dbname)
  File "/app/myFramework/source/posgresql/connect.py", line 6, in getCursor
    conn = psycopg2.connect(database = dbname,
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres_db" (172.20.0.5), port 9999 failed: FATAL:  database "0    dvdrental
Name: sourcedbname, dtype: object" does not exist

[2024-07-08T18:26:45.298+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=SqlToStaging, task_id=store, execution_date=20240707T000000, start_date=20240708T182643, end_date=20240708T182645
[2024-07-08T18:26:45.484+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 3 for task store (connection to server at "postgres_db" (172.20.0.5), port 9999 failed: FATAL:  database "0    dvdrental
Name: sourcedbname, dtype: object" does not exist
; 89)
[2024-07-08T18:26:45.547+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-07-08T18:26:45.668+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-08T18:32:10.742+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:32:10.766+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:32:10.772+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-07-08T18:32:10.812+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): store> on 2024-07-07 00:00:00+00:00
[2024-07-08T18:32:10.835+0000] {standard_task_runner.py:60} INFO - Started process 85 to run task
[2024-07-08T18:32:10.840+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'SqlToStaging', 'store', 'scheduled__2024-07-07T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpvbuwt8co']
[2024-07-08T18:32:10.855+0000] {standard_task_runner.py:88} INFO - Job 4: Subtask store
[2024-07-08T18:32:11.089+0000] {task_command.py:423} INFO - Running <TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [running]> on host 7f77c3b92645
[2024-07-08T18:32:11.661+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='SqlToStaging' AIRFLOW_CTX_TASK_ID='store' AIRFLOW_CTX_EXECUTION_DATE='2024-07-07T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-07T00:00:00+00:00'
[2024-07-08T18:32:11.666+0000] {logging_mixin.py:188} INFO - dvtobv :dag_id
[2024-07-08T18:32:11.673+0000] {logging_mixin.py:188} INFO - table: store
[2024-07-08T18:32:11.683+0000] {logging_mixin.py:188} INFO - 
 stage: SqlToStaging 
 table:store
[2024-07-08T18:32:11.703+0000] {logging_mixin.py:188} INFO - select *  from etlconf.Etl_Process_Mapping where SourceTableName = 'store' and Stage = 'SqlToStaging'
[2024-07-08T18:32:12.746+0000] {logging_mixin.py:188} INFO - 0    dvdrental
Name: sourcedbname, dtype: object 0    store
Name: sourcetablename, dtype: object 0    public
Name: sourceschema, dtype: object
[2024-07-08T18:32:12.771+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-07-08T18:32:12.800+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=SqlToStaging, task_id=store, execution_date=20240707T000000, start_date=20240708T183210, end_date=20240708T183212
[2024-07-08T18:32:12.874+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-07-08T18:32:13.019+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-08T18:35:59.012+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:35:59.051+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [queued]>
[2024-07-08T18:35:59.058+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-07-08T18:35:59.082+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): store> on 2024-07-07 00:00:00+00:00
[2024-07-08T18:35:59.090+0000] {standard_task_runner.py:60} INFO - Started process 101 to run task
[2024-07-08T18:35:59.097+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'SqlToStaging', 'store', 'scheduled__2024-07-07T00:00:00+00:00', '--job-id', '16', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpn7t8chwc']
[2024-07-08T18:35:59.109+0000] {standard_task_runner.py:88} INFO - Job 16: Subtask store
[2024-07-08T18:35:59.280+0000] {task_command.py:423} INFO - Running <TaskInstance: SqlToStaging.store scheduled__2024-07-07T00:00:00+00:00 [running]> on host a9b6fbb697c1
[2024-07-08T18:35:59.487+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='SqlToStaging' AIRFLOW_CTX_TASK_ID='store' AIRFLOW_CTX_EXECUTION_DATE='2024-07-07T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-07T00:00:00+00:00'
[2024-07-08T18:35:59.490+0000] {logging_mixin.py:188} INFO - dvtobv :dag_id
[2024-07-08T18:35:59.494+0000] {logging_mixin.py:188} INFO - table: store
[2024-07-08T18:35:59.495+0000] {logging_mixin.py:188} INFO - 
 stage: SqlToStaging 
 table:store
[2024-07-08T18:35:59.496+0000] {logging_mixin.py:188} INFO - select *  from etlconf.Etl_Process_Mapping where SourceTableName = 'store' and Stage = 'SqlToStaging'
[2024-07-08T18:35:59.499+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag.py", line 22, in process_table
    return  execute(table, stage)
  File "/app/main.py", line 25, in execute
    SqlToStaging().create_full().some_function(table, stage)
  File "/app/concretestage/SQLToStaging.py", line 31, in some_function
    df = get_data_from_conf_table(f"{table}", f"{stage}")
  File "/app/myFramework/utils/utils.py", line 138, in get_data_from_conf_table
    cur = conn.getCursor("postgres")
  File "/app/myFramework/source/posgresql/connect.py", line 6, in getCursor
    conn = psycopg2.connect(database = dbname,
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "0.0.0.0", port 9999 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

[2024-07-08T18:36:00.667+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=SqlToStaging, task_id=store, execution_date=20240707T000000, start_date=20240708T183559, end_date=20240708T183600
[2024-07-08T18:36:00.683+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 16 for task store (connection to server at "0.0.0.0", port 9999 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
; 101)
[2024-07-08T18:36:00.718+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-07-08T18:36:00.750+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
